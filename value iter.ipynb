{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def value_iteration(\n",
    "    max_O=5,   # max orders to track (O1,O2,O3)\n",
    "    max_A=5,   # max cabbages to track (A1,A2)\n",
    "    p_cancel=0.15,\n",
    "    gamma=1.0,  \n",
    "    theta=1e-6,\n",
    "    max_iter=10000,\n",
    "    track_deltas=True\n",
    "):\n",
    "    \"\"\"\n",
    "    NumPy-optimized Value Iteration for the cabbage merchant MDP.\n",
    "    Uses array indexing instead of dictionary lookups for better performance.\n",
    "    \"\"\"\n",
    "    # Initialize value function and policy using numpy arrays\n",
    "    # Shape: (max_O+1, max_O+1, max_O+1, max_A+1, max_A+1, 2, 2)\n",
    "    V = np.zeros((max_O+1, max_O+1, max_O+1, max_A+1, max_A+1, 2, 2))\n",
    "    policy = np.zeros((max_O+1, max_O+1, max_O+1, max_A+1, max_A+1, 2, 2, 2), dtype=int)\n",
    "    # policy has extra dimension of size 2 to store (accept, purchase) pairs\n",
    "    \n",
    "    # All possible actions\n",
    "    actions = [(accept, purchase) \n",
    "              for accept in [0, 1] \n",
    "              for purchase in range(6)]\n",
    "\n",
    "    def next_state_and_reward(state_idx, action):\n",
    "        \"\"\"\n",
    "        Given state indices and action, return next state indices and rewards.\n",
    "        \"\"\"\n",
    "        O1, O2, O3, A1, A2, c1, c2 = state_idx\n",
    "        accept, purchase = action\n",
    "\n",
    "        # Compute immediate reward\n",
    "        arriving_cabbages = A1 if c1 == 0 else 0\n",
    "        delivered = min(O1, arriving_cabbages)\n",
    "        missed = O1 - delivered\n",
    "        immediate_reward = 4 * delivered - missed - purchase\n",
    "\n",
    "        # Compute next state components\n",
    "        next_O1 = O2\n",
    "        next_O2 = O3\n",
    "        next_O3 = min(O3 + accept, max_O)\n",
    "        next_A1 = A2\n",
    "        next_A2 = min(purchase, max_A)\n",
    "        next_c1 = c2\n",
    "\n",
    "        # Return transitions for both possible values of next_c2\n",
    "        transitions = []\n",
    "        # Case 1: Not canceled (c2=0)\n",
    "        s_ok = (next_O1, next_O2, next_O3, next_A1, next_A2, next_c1, 0)\n",
    "        transitions.append((s_ok, 1-p_cancel, immediate_reward))\n",
    "        # Case 2: Canceled (c2=1)\n",
    "        s_cx = (next_O1, next_O2, next_O3, next_A1, next_A2, next_c1, 1)\n",
    "        transitions.append((s_cx, p_cancel, immediate_reward))\n",
    "\n",
    "        return transitions\n",
    "\n",
    "    # Value Iteration main loop\n",
    "    deltas = []\n",
    "    for iteration in tqdm(range(max_iter), desc=\"Value Iteration\"):\n",
    "        delta = 0.0\n",
    "        # Use numpy's ndindex to iterate over all state indices\n",
    "        for state_idx in np.ndindex(V.shape):\n",
    "            old_value = V[state_idx]\n",
    "            best_value = float('-inf')\n",
    "            best_act = (0, 0)\n",
    "\n",
    "            for action in actions:\n",
    "                outcomes = next_state_and_reward(state_idx, action)\n",
    "                q_sa = 0.0\n",
    "                for (next_state_idx, prob, reward) in outcomes:\n",
    "                    q_sa += prob * (reward + gamma * V[next_state_idx])\n",
    "\n",
    "                if q_sa > best_value:\n",
    "                    best_value = q_sa\n",
    "                    best_act = action\n",
    "\n",
    "            delta = max(delta, abs(best_value - old_value))\n",
    "            V[state_idx] = best_value\n",
    "            policy[state_idx] = best_act\n",
    "\n",
    "        deltas.append(delta)\n",
    "        if delta < theta:\n",
    "            print(f\"Converged in {iteration} iterations.\")\n",
    "            break\n",
    "\n",
    "    if track_deltas:\n",
    "        return V, policy, deltas\n",
    "    else:\n",
    "        return V, policy, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the optimized value iteration\n",
    "    V, policy, deltas = value_iteration(\n",
    "        max_O=5,\n",
    "        max_A=5,\n",
    "        p_cancel=0.15,\n",
    "        gamma=1.0,\n",
    "        theta=1e-8,\n",
    "        max_iter=10000,\n",
    "        track_deltas=True\n",
    "    )\n",
    "\n",
    "    # Plot convergence\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(deltas, label=\"Delta per iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Max Value Update (Delta)\")\n",
    "    plt.title(\"Value Iteration Convergence (NumPy Optimized)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Show optimal actions for sample states\n",
    "    sample_states = [\n",
    "        (0,0,0,0,0,0,0),\n",
    "        (1,0,0,1,0,0,0),\n",
    "        (2,2,0,2,1,0,1)\n",
    "    ]\n",
    "    for s in sample_states:\n",
    "        print(f\"State {s}: best action = {tuple(policy[s])}, value = {V[s]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After running value_iteration:\n",
    "np.save(\"value_function.npy\", V)\n",
    "np.save(\"policy.npy\", policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Later, you can load them without re-running everything:\n",
    "V = np.load(\"value_function.npy\")\n",
    "policy = np.load(\"policy.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_days=100, total profit=551.00, average profit/day=5.510\n",
      "For n_days=1000, total profit=5774.00, average profit/day=5.774\n",
      "For n_days=5000, total profit=28293.00, average profit/day=5.659\n",
      "For n_days=10000, total profit=55823.00, average profit/day=5.582\n",
      "For n_days=20000, total profit=111800.00, average profit/day=5.590\n",
      "For n_days=50000, total profit=280043.00, average profit/day=5.601\n",
      "For n_days=100000, total profit=557676.00, average profit/day=5.577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 1) Load the stored policy\n",
    "policy = np.load(\"policy.npy\")\n",
    "\n",
    "# 2) Define a simulation function matching your MDP logic.\n",
    "def simulate_policy_1day_cancellation(policy, \n",
    "                                      n_days=1000, \n",
    "                                      p_cancel=0.15, \n",
    "                                      max_O=5, \n",
    "                                      max_A=5, \n",
    "                                      seed=42):\n",
    "    \"\"\"\n",
    "    Simulates day-by-day using a policy array:\n",
    "      policy.shape = (max_O+1, max_O+1, max_O+1, max_A+1, max_A+1, 2, 2, 2)\n",
    "\n",
    "    State indices: (O1, O2, O3, A1, A2, c1, c2).\n",
    "      c1=1 => tomorrow's arrival is canceled, c1=0 => it will arrive\n",
    "      c2=1 => day-after-tomorrow's arrival is canceled, c2=0 => it will arrive\n",
    "\n",
    "    The policy entry: policy[O1, O2, O3, A1, A2, c1, c2] = [accept, purchase]\n",
    "    where 'accept' in {0,1}, 'purchase' in range(6) if we followed your code.\n",
    "\n",
    "    Returns:\n",
    "      total_profit, daily_profits\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Start state: (0,0,0,0,0, c1=0, c2=0)\n",
    "    state = (0, 0, 0, 0, 0, 0, 0)\n",
    "    (O1, O2, O3, A1, A2, c1, c2) = state\n",
    "\n",
    "    daily_profits = []\n",
    "    total_profit = 0.0\n",
    "\n",
    "    for day in range(n_days):\n",
    "        # 1) Retrieve policy decision\n",
    "        #    If indices go out of bounds, clamp them so we don't crash.\n",
    "        #    (Or do a check; here we clamp to ensure we don't exceed array dims.)\n",
    "        sO1 = min(O1, max_O)\n",
    "        sO2 = min(O2, max_O)\n",
    "        sO3 = min(O3, max_O)\n",
    "        sA1 = min(A1, max_A)\n",
    "        sA2 = min(A2, max_A)\n",
    "        sc1 = 1 if c1 != 0 else 0\n",
    "        sc2 = 1 if c2 != 0 else 0\n",
    "\n",
    "        # policy array holds [accept, purchase]\n",
    "        accept, purchase = policy[sO1, sO2, sO3, sA1, sA2, sc1, sc2]\n",
    "\n",
    "        # 2) If c1=0 => A1 arrives, else 0\n",
    "        arriving_cabbages = A1 if (c1 == 0) else 0\n",
    "\n",
    "        # Deliver to O1\n",
    "        delivered = min(O1, arriving_cabbages)\n",
    "        missed = O1 - delivered\n",
    "\n",
    "        # Daily profit\n",
    "        day_profit = 4*delivered - missed - purchase\n",
    "        total_profit += day_profit\n",
    "        daily_profits.append(day_profit)\n",
    "\n",
    "        # Shift states:\n",
    "        next_O1 = O2\n",
    "        next_O2 = O3\n",
    "        next_O3 = O3 + accept\n",
    "        if next_O3 > max_O:\n",
    "            next_O3 = max_O\n",
    "\n",
    "        next_A1 = A2\n",
    "        next_A2 = purchase if purchase <= max_A else max_A\n",
    "\n",
    "        next_c1 = c2  # shift c2 -> c1\n",
    "\n",
    "        # Draw new c2\n",
    "        if random.random() < p_cancel:\n",
    "            next_c2 = 1\n",
    "        else:\n",
    "            next_c2 = 0\n",
    "\n",
    "        # Update state\n",
    "        state = (next_O1, next_O2, next_O3, next_A1, next_A2, next_c1, next_c2)\n",
    "        (O1, O2, O3, A1, A2, c1, c2) = state\n",
    "\n",
    "    return total_profit, daily_profits\n",
    "\n",
    "\n",
    "# 3) Test with multiple day ranges\n",
    "day_ranges = [100, 1000, 5000, 10000, 20000, 50000, 100000]\n",
    "for n in day_ranges:\n",
    "    total, daily_list = simulate_policy_1day_cancellation(\n",
    "        policy=policy,\n",
    "        n_days=n,\n",
    "        p_cancel=0.15,\n",
    "        max_O=5, \n",
    "        max_A=5, \n",
    "        seed=42\n",
    "    )\n",
    "    avg_profit = total / n\n",
    "    print(f\"For n_days={n}, total profit={total:.2f}, average profit/day={avg_profit:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
